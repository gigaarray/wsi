\begin{uebsp}
\begin{Exercise}[label=ex:11.1]
X und Y haben die gemeinsame Verteilung\\
\begin{center}
    \begin{tabular}{|c|c|c|c|c|}\hline
 & \multicolumn{4}{c|} {Y} \\\hline
X & 1 & 2 & 3 & 4 \\\hline
1 & 1/4 & 0 & 1/8 & 1/8 \\\hline
2 & 0 & 1/8 & 1/8 & 0 \\\hline
3 & 0 & 1/16 & 0 & 1/16 \\\hline
4 & 0 & 1/16 & 0 & 1/16 \\\hline
\end{tabular}
\end{center}

Bestimmen Sie $H(X), H(Y), H(X,Y), H(X|Y), H(Y|X), I(X,Y)$
\end{Exercise}
\begin{Answer}
\index{Bedingte Entropie!Beispiel}
\index{gemeinsame Entropie!Beispiel}
\index{Information!Beispiel}
Zuerst bestimmen wir die gemeinsame Entropie (aus der Tabelle die Zahlen lesen):\\
$\displaystyle H(X,Y) = \sum_{x, y}^{} p(x,y) log_2(\frac{1}{p(x,y)}) = 3$\\
Als n√§chstes bestimmen wir die Randverteilungen:\\
$\displaystyle P_X = (\frac{1}{2}, \frac{1}{4}, \frac{1}{8}, \frac{1}{8})$\\
und\\
$\displaystyle P_Y = (\frac{1}{4}, \frac{1}{4}, \frac{1}{4}, \frac{1}{4})$\\
Daraus ergeben sich die Entropien:\\
$\displaystyle H(X) = \sum_{i=1}^{4} P_{Xi} log_2\left(\frac{1}{P_{Xi}}\right) =\frac{1}{2}\log_2\left(\frac{1}{\frac{1}{2}}\right)+\frac{1}{4}\log_2\left(\frac{1}{\frac{1}{4}}\right)+\frac{1}{8}\log_2\left(\frac{1}{\frac{1}{8}}\right)+\frac{1}{8}\log_2\left(\frac{1}{\frac{1}{8}}\right)=\frac{1}{2}\log_2(2)+\frac{1}{4}\log_2(4)+\frac{1}{4}\log_2(8)=\frac{7}{4}$\\
und
$H(Y) = 2$\\

Das ist alles um die bedingten Entropien sowie die Information zwischen X und Y zu berechnen:\\
$H(X|Y) = H(X,Y) - H(Y) = 1$\\
$\displaystyle H(X|Y) = H(X,Y) - H(X) = \frac{5}{4}$\\
$\displaystyle I(X,Y) = H(X) - H(X|Y) = \frac{3}{4}$
\end{Answer}
\end{uebsp}
